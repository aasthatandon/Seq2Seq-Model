{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f69169d-0683-4597-9432-838855349f39",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf47125f-008d-41b2-af68-05ab7a080d73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers import LSTM, Input, Dense,Embedding\n",
    "from keras.models import Model,load_model\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "pad_sequences = keras.preprocessing.sequence.pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b7c61-59d9-419a-a891-19de96b71418",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c7d47b-865f-49be-8a1b-dfccc4fa340e",
   "metadata": {},
   "source": [
    "This is a shell command that downloads a zip file containing French-English data from a website and extracts its contents using the unzip utility. The first command downloads the file, and the second command extracts it into the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9ef99f4-4002-4da0-931a-506324e2ae36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Archive:  fra-eng.zip',\n",
       " 'replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL',\n",
       " '(EOF or read error, treating as \"[N]one\" ...)']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
    "!!unzip fra-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebf16ae-43bf-4808-9f9d-0014342a26fb",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5fe5b-7afd-4405-8090-e079140d1719",
   "metadata": {},
   "source": [
    "This function reads text data from a file and processes it to extract input and target texts, and creates sets of input and target characters. It returns these data for further processing. The input and target texts are stored in separate lists, while the input and target characters are stored in separate sets. The function also adds a start sequence character and an end sequence character to the target texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a4248a8-ccd9-4ed7-bd1d-736e0d335671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the data txt file on disk.\n",
    "data_path = \"fra.txt\"\n",
    "\n",
    "num_samples = 20000  # Number of samples to train on.\n",
    "\n",
    "# Vectorize the data.\n",
    "def read_and_vectorize_data(data_path):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    input_characters = set()\n",
    "    target_characters = set()\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.read().split(\"\\n\")\n",
    "\n",
    "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "        input_text, target_text, _ = line.split(\"\\t\")\n",
    "        # We use \"tab\" as the \"start sequence\" character\n",
    "        # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "        target_text = \"\\t\" + target_text + \"\\n\"\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "        \n",
    "        input_text = set(input_text)\n",
    "        input_characters = input_characters.union(input_text)\n",
    "        \n",
    "        target_text = set(target_text)\n",
    "        target_characters = target_characters.union(target_text)\n",
    "            \n",
    "    return input_texts, target_texts, input_characters, target_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f16c78-1140-4cee-b9d5-1cd1013a3ef4",
   "metadata": {},
   "source": [
    "This code processes the output of read_and_vectorize_data by computing statistics and generating dictionaries for the input and target characters. It prints information about the data, including the number of samples, the number of unique input and output tokens, and the maximum sequence length for inputs and outputs. It also generates dictionaries for the input and target characters using dictionary comprehension, which will be used later to convert the input and target texts to numerical sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "effe1109-1597-4687-acba-2efa9c2a9c57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 20000\n",
      "Number of unique input tokens: 74\n",
      "Number of unique output tokens: 100\n",
      "Max sequence length for inputs: 17\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "input_texts, target_texts, input_characters, target_characters = read_and_vectorize_data(\n",
    "    data_path=data_path\n",
    ")\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "\n",
    "# Generate dictionary for characters\n",
    "input_token_index = {char: i for i, char in enumerate(input_characters)}\n",
    "target_token_index = {char: i for i, char in enumerate(target_characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df298e-b57f-4277-817c-2958ea3d7847",
   "metadata": {},
   "source": [
    "This function creates input/target data arrays for the model. It initializes three numpy arrays with zeros, and then loops through each input/target text pair. It fills in the encoder_input_data array with one-hot encoded characters from the input text. It fills in the decoder_input_data and decoder_target_data arrays with one-hot encoded characters from the target text, where the decoder_target_data array is shifted by one character to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c9ce671-5419-47ee-b4ae-81388361ea90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare input/target data for model\n",
    "def create_data_arrays():\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    "    )\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    "    )\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "        encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "\n",
    "        for t, char in enumerate(target_text):\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "\n",
    "        decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "        decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
    "        \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51ed340c-2784-48a4-90c0-2cc09f7196bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = create_data_arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca0ede-520d-47eb-8952-4f9e6a8b4beb",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379dcd64-b246-4626-a8a6-c263e06a3529",
   "metadata": {},
   "source": [
    "This function builds a sequence-to-sequence model using an LSTM encoder-decoder architecture with one-hot encoding. The encoder inputs are processed by an LSTM layer, and its output states are passed to the decoder. The decoder inputs are also passed through an LSTM layer with its own internal states. The final decoder output is processed by a dense layer with softmax activation to produce the predicted output sequence. The model is compiled using the RMSprop optimizer and categorical cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09ca3a53-3a1a-44ef-bebe-9c7e5d5ec866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    num_encoder_tokens, num_decoder_tokens, latent_dim=128, mname='char_model'\n",
    "):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=mname)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe9015-5d8f-487c-a245-0e8f494955b4",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e431985a-6e8b-4aed-9ec9-305db2fbea25",
   "metadata": {},
   "source": [
    "This code builds a model with a 256-dimensional LSTM encoder and decoder, and prints a summary of the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0df6b12f-f52a-43c3-8ba2-adea7202a362",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"char_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, None, 74)]   0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, None, 100)]  0           []                               \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 256),        338944      ['input_5[0][0]']                \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 256),  365568      ['input_6[0][0]',                \n",
      "                                 (None, 256),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 256)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, None, 100)    25700       ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 730,212\n",
      "Trainable params: 730,212\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    num_encoder_tokens=num_encoder_tokens,\n",
    "    num_decoder_tokens=num_decoder_tokens,\n",
    "    latent_dim=256,  # Latent dimensionality of the encoding space.\n",
    "    mname='char_model'\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120cf83-3cf5-4c89-b649-622a2ba30c91",
   "metadata": {},
   "source": [
    "This code trains the model using the input and target data arrays. It specifies the batch size and number of epochs, then calls the fit method on the model with the data arrays and training parameters as inputs. The model is also saved to disk with the name \"s2s\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd3258bb-64e0-4277-9bb8-b6c3314f2d39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 0.1327 - accuracy: 0.9576 - val_loss: 0.5220 - val_accuracy: 0.8788\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 9s 35ms/step - loss: 0.1309 - accuracy: 0.9583 - val_loss: 0.5296 - val_accuracy: 0.8779\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 9s 37ms/step - loss: 0.1292 - accuracy: 0.9586 - val_loss: 0.5314 - val_accuracy: 0.8788\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 10s 38ms/step - loss: 0.1277 - accuracy: 0.9591 - val_loss: 0.5329 - val_accuracy: 0.8784\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 10s 40ms/step - loss: 0.1258 - accuracy: 0.9596 - val_loss: 0.5364 - val_accuracy: 0.8788\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.1243 - accuracy: 0.9600 - val_loss: 0.5397 - val_accuracy: 0.8791\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 11s 45ms/step - loss: 0.1226 - accuracy: 0.9604 - val_loss: 0.5438 - val_accuracy: 0.8784\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 12s 46ms/step - loss: 0.1211 - accuracy: 0.9609 - val_loss: 0.5496 - val_accuracy: 0.8781\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.1197 - accuracy: 0.9614 - val_loss: 0.5525 - val_accuracy: 0.8783\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 10s 40ms/step - loss: 0.1183 - accuracy: 0.9617 - val_loss: 0.5515 - val_accuracy: 0.8781\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 0.1167 - accuracy: 0.9621 - val_loss: 0.5591 - val_accuracy: 0.8784\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 10s 40ms/step - loss: 0.1158 - accuracy: 0.9625 - val_loss: 0.5599 - val_accuracy: 0.8782\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 10s 40ms/step - loss: 0.1139 - accuracy: 0.9629 - val_loss: 0.5681 - val_accuracy: 0.8781\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.1129 - accuracy: 0.9633 - val_loss: 0.5675 - val_accuracy: 0.8785\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 10s 40ms/step - loss: 0.1115 - accuracy: 0.9636 - val_loss: 0.5691 - val_accuracy: 0.8777\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 10s 40ms/step - loss: 0.1102 - accuracy: 0.9640 - val_loss: 0.5721 - val_accuracy: 0.8774\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 10s 39ms/step - loss: 0.1090 - accuracy: 0.9643 - val_loss: 0.5758 - val_accuracy: 0.8778\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.1079 - accuracy: 0.9646 - val_loss: 0.5811 - val_accuracy: 0.8776\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 10s 40ms/step - loss: 0.1067 - accuracy: 0.9649 - val_loss: 0.5809 - val_accuracy: 0.8783\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.1053 - accuracy: 0.9654 - val_loss: 0.5864 - val_accuracy: 0.8767\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.1043 - accuracy: 0.9656 - val_loss: 0.5889 - val_accuracy: 0.8781\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.1029 - accuracy: 0.9661 - val_loss: 0.5945 - val_accuracy: 0.8775\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.1021 - accuracy: 0.9663 - val_loss: 0.5974 - val_accuracy: 0.8770\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 10s 42ms/step - loss: 0.1010 - accuracy: 0.9667 - val_loss: 0.5964 - val_accuracy: 0.8774\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.1001 - accuracy: 0.9669 - val_loss: 0.6006 - val_accuracy: 0.8776\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.0991 - accuracy: 0.9673 - val_loss: 0.6014 - val_accuracy: 0.8777\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.0979 - accuracy: 0.9675 - val_loss: 0.6047 - val_accuracy: 0.8779\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 10s 40ms/step - loss: 0.0969 - accuracy: 0.9679 - val_loss: 0.6082 - val_accuracy: 0.8771\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 0.0958 - accuracy: 0.9681 - val_loss: 0.6113 - val_accuracy: 0.8773\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.0947 - accuracy: 0.9685 - val_loss: 0.6138 - val_accuracy: 0.8773\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.0938 - accuracy: 0.9689 - val_loss: 0.6170 - val_accuracy: 0.8774\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.0931 - accuracy: 0.9689 - val_loss: 0.6203 - val_accuracy: 0.8764\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.0921 - accuracy: 0.9692 - val_loss: 0.6260 - val_accuracy: 0.8764\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 10s 42ms/step - loss: 0.0912 - accuracy: 0.9696 - val_loss: 0.6323 - val_accuracy: 0.8766\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 10s 42ms/step - loss: 0.0904 - accuracy: 0.9697 - val_loss: 0.6286 - val_accuracy: 0.8771\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 11s 42ms/step - loss: 0.0895 - accuracy: 0.9700 - val_loss: 0.6326 - val_accuracy: 0.8763\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 13s 50ms/step - loss: 0.0886 - accuracy: 0.9703 - val_loss: 0.6356 - val_accuracy: 0.8771\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0877 - accuracy: 0.9707 - val_loss: 0.6382 - val_accuracy: 0.8764\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 14s 54ms/step - loss: 0.0871 - accuracy: 0.9708 - val_loss: 0.6404 - val_accuracy: 0.8768\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0861 - accuracy: 0.9710 - val_loss: 0.6437 - val_accuracy: 0.8767\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0855 - accuracy: 0.9710 - val_loss: 0.6479 - val_accuracy: 0.8767\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0845 - accuracy: 0.9716 - val_loss: 0.6495 - val_accuracy: 0.8763\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 0.0836 - accuracy: 0.9719 - val_loss: 0.6495 - val_accuracy: 0.8765\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0832 - accuracy: 0.9720 - val_loss: 0.6530 - val_accuracy: 0.8768\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.0823 - accuracy: 0.9721 - val_loss: 0.6556 - val_accuracy: 0.8765\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 0.0817 - accuracy: 0.9724 - val_loss: 0.6572 - val_accuracy: 0.8771\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0808 - accuracy: 0.9727 - val_loss: 0.6656 - val_accuracy: 0.8766\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0802 - accuracy: 0.9729 - val_loss: 0.6661 - val_accuracy: 0.8764\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 11s 46ms/step - loss: 0.0794 - accuracy: 0.9731 - val_loss: 0.6647 - val_accuracy: 0.8766\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0787 - accuracy: 0.9732 - val_loss: 0.6694 - val_accuracy: 0.8764\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 0.0781 - accuracy: 0.9734 - val_loss: 0.6752 - val_accuracy: 0.8765\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 0.0774 - accuracy: 0.9736 - val_loss: 0.6796 - val_accuracy: 0.8758\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.0766 - accuracy: 0.9740 - val_loss: 0.6754 - val_accuracy: 0.8766\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 0.0758 - accuracy: 0.9741 - val_loss: 0.6814 - val_accuracy: 0.8765\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.0754 - accuracy: 0.9742 - val_loss: 0.6821 - val_accuracy: 0.8768\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0747 - accuracy: 0.9744 - val_loss: 0.6837 - val_accuracy: 0.8761\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 13s 51ms/step - loss: 0.0741 - accuracy: 0.9746 - val_loss: 0.6870 - val_accuracy: 0.8766\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 13s 51ms/step - loss: 0.0732 - accuracy: 0.9748 - val_loss: 0.6903 - val_accuracy: 0.8764\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0727 - accuracy: 0.9751 - val_loss: 0.6940 - val_accuracy: 0.8758\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0726 - accuracy: 0.9750 - val_loss: 0.7000 - val_accuracy: 0.8754\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0717 - accuracy: 0.9752 - val_loss: 0.6967 - val_accuracy: 0.8760\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0714 - accuracy: 0.9753 - val_loss: 0.7012 - val_accuracy: 0.8764\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0708 - accuracy: 0.9757 - val_loss: 0.7063 - val_accuracy: 0.8752\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0704 - accuracy: 0.9757 - val_loss: 0.7051 - val_accuracy: 0.8758\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 0.0695 - accuracy: 0.9760 - val_loss: 0.7140 - val_accuracy: 0.8754\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 13s 50ms/step - loss: 0.0690 - accuracy: 0.9761 - val_loss: 0.7087 - val_accuracy: 0.8761\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 0.0682 - accuracy: 0.9764 - val_loss: 0.7139 - val_accuracy: 0.8755\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0677 - accuracy: 0.9766 - val_loss: 0.7168 - val_accuracy: 0.8758\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0673 - accuracy: 0.9766 - val_loss: 0.7180 - val_accuracy: 0.8760\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0670 - accuracy: 0.9767 - val_loss: 0.7247 - val_accuracy: 0.8759\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0664 - accuracy: 0.9768 - val_loss: 0.7247 - val_accuracy: 0.8751\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0661 - accuracy: 0.9769 - val_loss: 0.7284 - val_accuracy: 0.8755\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 0.0656 - accuracy: 0.9772 - val_loss: 0.7278 - val_accuracy: 0.8758\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 0.0649 - accuracy: 0.9774 - val_loss: 0.7245 - val_accuracy: 0.8754\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 13s 52ms/step - loss: 0.0644 - accuracy: 0.9776 - val_loss: 0.7307 - val_accuracy: 0.8759\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 13s 50ms/step - loss: 0.0642 - accuracy: 0.9776 - val_loss: 0.7297 - val_accuracy: 0.8765\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 13s 51ms/step - loss: 0.0636 - accuracy: 0.9778 - val_loss: 0.7405 - val_accuracy: 0.8751\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.0631 - accuracy: 0.9779 - val_loss: 0.7402 - val_accuracy: 0.8754\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 13s 51ms/step - loss: 0.0628 - accuracy: 0.9780 - val_loss: 0.7445 - val_accuracy: 0.8754\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0624 - accuracy: 0.9781 - val_loss: 0.7404 - val_accuracy: 0.8751\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0620 - accuracy: 0.9782 - val_loss: 0.7414 - val_accuracy: 0.8756\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0616 - accuracy: 0.9783 - val_loss: 0.7428 - val_accuracy: 0.8754\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0609 - accuracy: 0.9785 - val_loss: 0.7522 - val_accuracy: 0.8746\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 0.0608 - accuracy: 0.9787 - val_loss: 0.7501 - val_accuracy: 0.8752\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 0.0603 - accuracy: 0.9787 - val_loss: 0.7535 - val_accuracy: 0.8754\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 13s 54ms/step - loss: 0.0598 - accuracy: 0.9790 - val_loss: 0.7535 - val_accuracy: 0.8750\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0595 - accuracy: 0.9788 - val_loss: 0.7551 - val_accuracy: 0.8755\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 13s 51ms/step - loss: 0.0590 - accuracy: 0.9790 - val_loss: 0.7565 - val_accuracy: 0.8743\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 13s 51ms/step - loss: 0.0588 - accuracy: 0.9793 - val_loss: 0.7596 - val_accuracy: 0.8751\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 13s 50ms/step - loss: 0.0583 - accuracy: 0.9793 - val_loss: 0.7595 - val_accuracy: 0.8752\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 14s 54ms/step - loss: 0.0581 - accuracy: 0.9794 - val_loss: 0.7581 - val_accuracy: 0.8757\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 13s 52ms/step - loss: 0.0577 - accuracy: 0.9795 - val_loss: 0.7648 - val_accuracy: 0.8748\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 13s 52ms/step - loss: 0.0572 - accuracy: 0.9796 - val_loss: 0.7656 - val_accuracy: 0.8753\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 13s 52ms/step - loss: 0.0571 - accuracy: 0.9797 - val_loss: 0.7666 - val_accuracy: 0.8756\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.0568 - accuracy: 0.9798 - val_loss: 0.7728 - val_accuracy: 0.8742\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 0.0564 - accuracy: 0.9799 - val_loss: 0.7729 - val_accuracy: 0.8748\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 15s 58ms/step - loss: 0.0559 - accuracy: 0.9801 - val_loss: 0.7686 - val_accuracy: 0.8754\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.0556 - accuracy: 0.9802 - val_loss: 0.7739 - val_accuracy: 0.8742\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 13s 54ms/step - loss: 0.0552 - accuracy: 0.9803 - val_loss: 0.7789 - val_accuracy: 0.8744\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.0550 - accuracy: 0.9804 - val_loss: 0.7802 - val_accuracy: 0.8748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s/assets\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64  # Batch size \n",
    "epochs = 100  # Number of epochs\n",
    "\n",
    "history = model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "# Save model\n",
    "model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c3d980-fa7d-4354-90df-9e075e9ad354",
   "metadata": {},
   "source": [
    "\n",
    "### Run inference (sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f534c2dc-fb07-4904-af09-07f37870092f",
   "metadata": {},
   "source": [
    "This code defines two sampling models to be used during inference, based on the previously trained model. The encoder model takes input text and returns the encoder states, while the decoder model takes in the decoder inputs and initial decoder states, and returns the predicted outputs and updated decoder states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67ba5733-9f2b-4288-80c2-25e2b98e427e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "# Restore the model and construct the encoder and decoder.\n",
    "def load_saved_model(model_ckpt=\"s2s\",latent_dim=256):\n",
    "    model = keras.models.load_model(model_ckpt)\n",
    "\n",
    "    encoder_inputs = model.input[0]  # input_1\n",
    "    encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "    encoder_states = [state_h_enc, state_c_enc]\n",
    "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_inputs = model.input[1]  # input_2\n",
    "    decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_lstm = model.layers[3]\n",
    "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs\n",
    "    )\n",
    "    decoder_states = [state_h_dec, state_c_dec]\n",
    "    decoder_dense = model.layers[4]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = keras.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    "    )\n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa15c90a-e119-47f5-80af-50d27cd52691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_model, decoder_model = load_saved_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0684cb9-c6f4-4d90-965e-ea59b4d6fb18",
   "metadata": {},
   "source": [
    "This code defines a function to decode a sequence using the encoder and decoder models. The input sequence is first encoded by the encoder model to get the initial states. The function then uses a loop to generate the output sequence one character at a time using the decoder model and the previous output as input. The loop terminates when the end-of-sequence character is generated or the maximum sequence length is reached. Finally, the function returns the decoded sequence as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dce21bbf-ad89-4396-9033-71efb8a98a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = {i: char for char, i in input_token_index.items()}\n",
    "reverse_target_char_index = {i: char for char, i in target_token_index.items()}\n",
    "\n",
    "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f5fa4-5221-4d6a-86cc-20ff86ccfcb8",
   "metadata": {},
   "source": [
    "This code generates translations using the trained encoder and decoder models. It takes an input sentence, encodes it with the encoder model, decodes it with the decoder model, and prints the original, decoded, and expected sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "449ecfc3-5662-4776-bf85-00d5c5a41e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 292ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Ça alors !\n",
      "\n",
      "Original sentence: \tWah !\n",
      "\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: Baisse-toi !\n",
      "\n",
      "Original sentence: \tÀ terre !\n",
      "\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: Baisse-toi !\n",
      "\n",
      "Original sentence: \tBaisse-toi !\n",
      "\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: Baisse-toi !\n",
      "\n",
      "Original sentence: \tBaissez-vous !\n",
      "\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Au feu !\n",
      "\n",
      "Original sentence: \tAu feu !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(25, 30):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", input_texts[seq_index])\n",
    "    print(\"Decoded sentence:\", decoded_sentence)\n",
    "    print(\"Original sentence:\", target_texts[seq_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
